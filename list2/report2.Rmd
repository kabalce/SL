---
title: "Report 2"
subtitle: "Selecting Features with Information Criteria"
author: "Klaudia Balcer"
date: "12/17/2021"
output: 
  pdf_document:
    extra_dependencies: ["bbm", "caption", "tabularx", "booktabs", "graphicx", "float", "textpos"]
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(fig.width = 10, fig.height = 5)
library(ggplot2)
library(knitr)
library(reshape2)
library(sfsmisc)
library(stringr)
library(bigstep)

set.seed(2022)
```

\pagebreak

# Introduction

When considering statistical regression models, our goal is to get good predictions for new variables and detect real relationships in data. As it is about linear regression, we need to select variables well (in order to find relationships) and estimate the coefficients (to get good predictions for new observations). One can use various methods to select the variables. In the report we will study the properties of several information criteria, both using real and simulated data.

The first two tasks are simulation studies. We will consider a model with five sufficient variables and a different (rather large) number of insufficient variables. First, we will see how the prediction error behaves when selecting insufficient variables. In the second task, we will compare the properties of different information criteria. In the last task, we will make a case study with data from the genetics domain.

<!-- \pagebreak -->

# Task 1

We will simulate the design matrix from the normal distribution:

$$X_{1000 \times 950} \sim \mathcal N \Big(0, \sigma=\frac {1} {\sqrt 1000} \Big)$$ i.i.d.

The real vector of coefficients have five signals of strength tree and different number ($q - 5$) of zero entries:
$$\beta = (3, 3, 3, 3, 3, 0, \ldots, 0)^T$$.

Random noise will be added to the data:

$\epsilon \sim \mathcal N (\vec 0, \mathbb I)$.

The length of the vector of coefficients:

- $q = 2, 5, 10, 100, 500, 950$.

The number of repetitions of the experiment: 100.


```{r 1_setup}
n <- 1000
p <- 950
qs <- c(2, 5, 10, 100, 500, 950)
bet <- c(rep(3, 5), rep(0, p - 5))
```

#### Prediction Error

As discussed in the introduction, looking only at the fit of the model may lead to overfitting. It is important to look at how our model behaves for new data. A good measurement is the prediction error. It is the expected norm of the difference between the estimated responses and values of the response variable generated by the same design matrix and a new error term. In a perfect situation, the MSE of the estimators should be the same for the data used for training and the new data (obtained by using a new error term). We will observe the prediction error and its estimators for different $q$s.

**Prediction error**:

$$PE = \mathbb E \Vert X(\beta - \hat \beta) + \epsilon ^* \Vert^2,$$
where  $\epsilon ^* \sim \mathcal N (\vec 0, \mathbb I)$ is a new noise vector.

**SURE estimator of the PE (in orthogonal design)**:

- $\sigma$ known:
  $$\widehat {PE}_1 = RSS + 2 \sigma^2p$$
  
- $\sigma$ unknown:
  $$\widehat {PE}_2 = RSS + 2 \hat\sigma^2p = \frac {(n+p) RSS}{n-p}$$
  
**Leave-one-out CV PE estimator**:

$$\widehat {PE}_3 = \sum_i^n \Big(\frac {Y_i - \hat Y_i} {1 - H_{i, i}} \Big)^2,$$ where $H=X(X'X)^{-1}X'$ is the projection matrix.


```{r 1_PE}
RSS <- function(X, Y, b_hat) {
  sum((Y - X %*% b_hat) ^2)
}

PE <- function(X, b_hat, b, eps_new) {
 sum((X %*% (b - b_hat) + eps_new) ^2)
}

PE_1 <- function(X, Y, b_hat, sig) {
  p <- dim(X)[2]
  RSS(X, Y, b_hat) + 2 * p * sig
}

PE_2 <- function(X, Y, b_hat) {
  n <- dim(X)[1]
  p <- dim(X)[2]
  RSS(X, Y, b_hat) * (n + p) / (n - p)
}

PE_CV <- function(X, Y, b_hat) {
  H <- X %*% solve(t(X) %*% X) %*% t(X)
  sum(((Y - X %*% b_hat) / (1 - diag(H))) ^2)
}

AIC1 <- function(X, Y, b_hat, sig) {
  n <- dim(X)[1]
  p <- dim(X)[2]
  -n * log(sqrt(2 * pi)) - RSS(X, Y, b_hat) / 2 / sig - p 
}

AIC2 <- function(X, Y, b_hat) {
  n <- dim(X)[1]
  p <- dim(X)[2]
  -n * log(sqrt(2 * pi)) - n/2 * log(RSS(X, Y, b_hat)) - p
}
```


```{r 1_simulation}
calculations1 <- function(X, Y, b, eps_new, sig) {  
  n <- dim(X)[1]
  p <- dim(X)[2]
  model <- lm(Y ~ X - 1)
  b_hat <- model$coefficients
  rss <- RSS(X, Y, b_hat)
  c(PE=PE(X, b_hat, b, eps_new), 
    PE1=PE_1(X, Y, b_hat, sig), 
    PE2=PE_2(X, Y, b_hat), 
    PE3=PE_CV(X, Y, b_hat), 
    AIC1=AIC1(X, Y, b_hat, sig), 
    AIC2=AIC2(X, Y, b_hat),
    FP_known=sum(abs(b_hat) > sqrt(2) & !b),
    FN_known=sum(!(abs(b_hat) > sqrt(2)) & b),
    FP_unknown=sum(abs(b_hat) > sqrt(rss/(n-p)*2) & !b),
    FN_unknown=sum(!(abs(b_hat) > sqrt(rss/(n-p)*2)) & b)) 
} 

step1 <- function() {
  X <- matrix(rnorm(n*p, 0, 1 / sqrt(n)), n, p)
  eps <- rnorm(n)
  Y <- X %*% bet + eps
  eps_new <- rnorm(1000)
  sapply(qs, function(q) calculations1(X[, 1:q], Y, bet[1:q], eps_new, sig=1))
}

# res1 <- replicate(100, step1())
# 
# saveRDS(res1, "results_full_1.RData")
```


```{r 1_results_PE}
res1 <- readRDS("results_full_1.RData")

PE1_df <- data.frame(t(res1["PE1", , ] - res1["PE", , ]))
colnames(PE1_df) <- qs
PE1_df["estimator"] <- "PE1"

PE2_df <- data.frame(t(res1["PE2", , ] - res1["PE", , ]))
colnames(PE2_df) <- qs
PE2_df["estimator"] <- "PE2"

PE3_df <- data.frame(t(res1["PE3", , ] - res1["PE", , ]))
colnames(PE3_df) <- qs
PE3_df["estimator"] <- "PE3"

PE_df <- melt(rbind(PE1_df, PE2_df, PE3_df))

ggplot(PE_df, aes(x=variable, y=value, fill=estimator)) +
  geom_boxplot() + 
  labs(title="Boxplots of bias of PE estimator") +
  scale_y_continuous(trans='log10')
```

When increasing the dimension of the design matrix (the number of columns $p$), the fit is always better. However, adding insufficient variables may cause the prediction properties to worsen despite better fit. Such a situation can be called overfitting. We can observe it at the prediction error estimators' boxplots.

#### AIC

Optimizing the Akkaike Infomation Criteria (AIC) corresponds with a decrease in the prediction error. The geenral formula of the criterion is: 

$$AIC(M_p) = ln \mathcal L (X, \hat \theta_{MLE}) - p,$$

where p is the length of the parameter vector ($\theta \in \mathbb R ^p$). 
  
When sigma is known, the AIC can be calculated using the formula:

$$AIC(M_k) = C_{n, \sigma} - \frac{RSS}{2\sigma^2} - k$$

Maximizing AIC is equivalent to minimizing $\frac{RSS}{2\sigma^2} + k$.


When sigma is unknown, the AIC can be calculated using the formula:

$$AIC(M_k) = C_{n} - \frac{n}{2}log(RSS) - k$$

Maximizing AIC is equivalent to minimizing $\frac{n}{2}log(RSS) + k$.
  
```{r 1_results_AIC}
AIC1_df <- data.frame(t(res1["AIC1", , ]))
colnames(AIC1_df) <- qs
AIC1_df["estimator"] <- "AIC_known"

AIC2_df <- data.frame(t(res1["AIC2", , ]))
colnames(AIC2_df) <- qs
AIC2_df["estimator"] <- "AIC_unnown"

AIC_df <- melt(rbind(AIC1_df, AIC2_df))

ggplot(melt(AIC1_df), aes(x=variable, y=value)) +
  geom_boxplot() + 
  labs(title="Boxplots of bias of AIC estimator",
       subtitle="for known sigma")

ggplot(melt(AIC2_df), aes(x=variable, y=value)) +
  geom_boxplot() + 
  labs(title="Boxplots of bias of AIC estimator",
       subtitle="for unknown sigma") 
```

When $\sigma$ is known, the AIC criterion leads the proper model (we obtain the largest AIC for the model with only significant variables). For unknown $\sigma$ the criterion has a strange aberration for $p=950$ and leads to the model with the most insufficient variables.


```{r}
calculations1_1 <- function(X, Y, b, eps_new, sig) {
  n <- dim(X)[1]
  p <- dim(X)[2]
  model <- lm(Y ~ X - 1)
  b_hat <- model$coefficients
  c(AIC1=AIC1(X, Y, b_hat, sig), 
    AIC2=AIC2(X, Y, b_hat))
}

step1_1 <- function() {
  X <- matrix(rnorm(n*p, 0, 1 / sqrt(n)), n, p)
  eps <- rnorm(n)
  Y <- X %*% bet + eps
  eps_new <- rnorm(1000)
  sapply(1:950, function(q) calculations1_1(as.matrix(X[, 1:q]), Y, bet[1:q], eps_new, sig=1))
}

# res1_1 <- replicate(100, step1_1())
```

\pagebreak

# Task 2

In this task we will consider the same setup as in the first one.

```{r 2_setup}
qs <- c(20, 100, 500, 950)
crits <- c(BIC=bic, AIC=aic, RIC=function(loglik, k, p) {2 * k * log(p) - 2 * loglik}, mBIC=mbic, mBIC2=mbic2)
```

```{r 2_simulation}
calculations2 <- function(X, Y, b, criterion) {
  d <- prepare_data(Y, X)
  model <- stepwise(d, criterion)
  
  vars <- rep(F, length(b))
  vars[as.numeric(model$model)] <- T
  
  coefs <- summary(model)$coefficients[, 1]
  b_hat <- rep(0, length(b))
  b_hat[as.numeric(model$model)] <- coefs[-1]
  
  c(FP=sum(vars & (b == 0)),
    FN=sum(!vars & (b > 0)),
    Reject=length(model$model),
    SSE=sum((X %*% (b - b_hat) - coefs[1]) ^2))
}

step2 <- function() {
  X <- matrix(rnorm(n*p, 0, 1 / sqrt(n)), n, p)
  eps <- rnorm(n)
  Y <- X %*% bet + eps
  
  sapply(qs, function(q)
    sapply(crits, function(crit) 
      calculations2(X[, 1:q], Y, bet[1:q], crit)))
}

# res2 <- replicate(100, step2())
# 
# saveRDS(res2, "results_full_2.RData")
```


Presented criteria have different properties. Akaike Information Criterion (AIC) selects the model with the best predictive properties. Bayesian Information Criterion (BIC) is consistent. Namely, with n growing to infinity, the probability of detecting the proper model converges to one. The Risk Inflation Criterion (RIC) controls the FDR (false discovery rate). The probability of type I error is very low when using RIC (much lower than for BIC and AIC). Modified versions of BIC (mBIC and mBIC2) were developed for the case when the number of variables is greater than the number of observations, which is not our case in this task. mBIC is ABOS (asymptotically Bayes optimal under sparsity) for extremely sparse signals and corresponds to Bonferroni's correction. mBIC2 adapts well to unknown sparsity (is ABOS for unknown sparsity) and corresponds to Benjamini-Hochberg's procedure.  

We used those methods to select the best model. However, checking $2^q$ models could be computationally very expensive. Thus we need to apply a procedure that leads to a good (even if not optimal) solution. Examples of such procedures are forward selection, backward elimination, and stepwise selection. 

In the simulations, we have used the stepwise procedure. The only exception is the Akaike Criterion for $p=950$ when the forward selection was used (due to the high time complexity).

In this task, we consider $n > p$. However, for $p=950$ the difference between $p$ and $n$ is minimal. We can observe how the criteria good for a lower number of variables (AIC, BIC, RIC) lose their desired (expected) properties. The results are to find in tables 1-5.



```{r 2_results, results='asis', fig.pos='H'}
res2 <- readRDS("results_full_2.RData")
res2_950 <- readRDS("results_950_2.RData")
res2_950AIC <- readRDS("results_950_AIC_2.RData")

prep_data2 <- function(i, metric_names, res) {
  tmp <- res[((i-1) * length(metric_names) + 1):(i * length(metric_names)), , ]
  
  tmp[3, , ] <- (tmp[3, , ] - tmp[1, , ]) / 5
  result <- sapply(1:(dim(tmp)[2]), function(j) 
    rowMeans(tmp[, j, ]))
  rownames(result) <- metric_names
  colnames(result) <- qs[1:(dim(result)[2])]
  result
}

dim_names <- c("FP", "FN", "power", "SSE")

res2_processed <- lapply(1:(dim(res2)[1] / length(dim_names)), function (i) prep_data2(i, dim_names, res2))

names(res2_processed) <- names(sapply(crits, function(c) substitute(c)))

res22_950 <- rbind(res2_950[1:4, 1, ], res2_950AIC[1:4, , ],
res2_950[5:16, , ])

dim(res22_950) <- c(20, 1, 100)

res22_950[3, , ] <- (res22_950[3, , ] - res22_950[1, , ]) / 5
res22_950[7, , ] <- (res22_950[7, , ] - res22_950[5, , ]) / 5
res22_950[11, , ] <- (res22_950[11, , ] - res22_950[9, , ]) / 5
res22_950[15, , ] <- (res22_950[15, , ] - res22_950[13, , ]) / 5

res22_950[19, , ] <- (res22_950[19, , ] - res22_950[17, , ]) / 5

results950 <- rowMeans(res22_950)

for( i in 1:length(res2_processed)) {
  n <- colnames(res2_processed[[names(res2_processed)[i]]] )
  res2_processed[[names(res2_processed)[i]]] <- cbind(res2_processed[[names(res2_processed)[i]]], results950[((i-1) * 4 + 1):(i*4)])
  colnames(res2_processed[[names(res2_processed)[i]]] ) <- c(n, 950)
}

for (c in names(res2_processed)) {
  cat('\n\n<!-- -->\n\n')
  print(kable(res2_processed[c], caption=paste0("Results for ", c, " criterion"), 
              digits=3, position = "!hb", booktabs = TRUE, 
              valign = 't', format = "latex"))
}
```

The BIC criterion makes quite many false discoveries for a big number of features. However, it holds its power (around 0.65) when the number of observations increases.  The AIC is much more powerful. It also makes many more false discoveries. The stepwise computations for AIC were very time-consuming, it was not possible to obtain results in a reasonable time for $n=950$. Thus, the results are presented for the forward selection. This procedure did not work well.  The RIC is very conservative. It controls the FDR independently of the value of $n$. The cost of this property is very low power for large $p$. The mBIC is also a very conservative criterion. On the one hand, its power is even lower than RIC's. On the other hand, mBIC makes almost no false discoveries. The second modification of the Bayesian information criterion worked better than the presented first. However, it is not dedicated to such a case. mBIC2 controls FDR. 

Additionally, below you can find the histograms for the number of false discoveries (FP) and the number of undiscovered signals (FN) for the Akaike criterion (in default setup with unknown sigma).


```{r, fig.width = 3, fig.height = 3}
FP <- data.frame(cbind(t(res2[5, , ]), res2_950AIC[1, , ]))
colnames(FP) <-as.character(qs)

FN <- data.frame(cbind(t(res2[6, , ]), res2_950AIC[2, , ]))
colnames(FN) <-as.character(qs)

ggplot(FP, aes(x=`20`)) + geom_histogram() + labs(title="Histogram of FP for p=20", subtitle ="sigma unknown")
ggplot(FN, aes(x=`20`)) + geom_histogram() + labs(title="Histogram of FN for p=20", subtitle ="sigma unknown") 
cat("\n\n")

ggplot(FP, aes(x=`100`)) + geom_histogram() + labs(title="Histogram of FP for p=100", subtitle = "sigma unknown")
ggplot(FN, aes(x=`100`)) + geom_histogram() + labs(title="Histogram of FN for p=100", subtitle = "sigma unknown")
cat("\n\n")

ggplot(FP, aes(x=`500`)) + geom_histogram() + labs(title="Histogram of FP for p=500", subtitle = "sigma unknown")
ggplot(FN, aes(x=`500`)) + geom_histogram() + labs(title="Histogram of FN for p=500", subtitle = "sigma unknown")
cat("\n\n")

ggplot(FP, aes(x=`950`)) + geom_histogram() + labs(title="Histogram of FPs for p=950", subtitle = "sigma unknown")
ggplot(FN, aes(x=`950`)) + geom_histogram() + labs(title="Histogram of FN for p=950", subtitle = "sigma unknown")
cat("\n\n")
```
\pagebreak

# Task 3

```{r}
d <- readRDS("realdata.RData")
d <- matrix(d[1:210, 1:3221], 210, 3221)
test_index <- sample(1:210, 30)

d_train <- d[-test_index, ]
d_test <- d[test_index, ]

d_bs <- prepare_data(d_train[, 1], d_train[, 2:(dim(d_train)[2])])

calculation3 <- function(criterion) {
  model <- stepwise(d_bs, criterion)
  
  coefs <- summary(model)$coefficients[, 1]
  b_hat <- rep(0, 3220)
  b_hat[as.numeric(model$model)] <- coefs[-1]
  
  preds <- d_test[, -1] %*% b_hat + coefs[1]
  sum((d_test[, 1] - preds)^2)
}

# res3 <- sapply(crits, calculation3)
# saveRDS(res3, "results_full_3.RData")
```

The third task is a case study. We work with data from the genetic domain. The number of features (3200) is much larger than the number of observations (210). Thus the traditional methods of selecting variables do not work well. Stepwise procedure was used with several criteria. The error of the predictions is significantly lower when using methods that control FDR (RIC, mBIC, mBIC2). The best results are obtained by RIC and mBIC2. 

```{r 3_results, results='asis'}
res3 <- readRDS("results_full_3.RData")

kable(t(data.frame(res3)), caption="Prediction Error for different criteria", digits=3, row.names = F)
```